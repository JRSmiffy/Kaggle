{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Titanic\" Kaggle Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/titanic\n",
    "\n",
    "## Overview\n",
    "Predicting survival for Titanic passengers, based on age, social class, gender, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method\n",
    "\n",
    "In this challenge, I am using the Random Forest Model.\n",
    "\n",
    "* define X (Features: \"Pclass\", \"Sex\", \"SibSp\", \"Parch\") & y (Features: \"Survived\")\n",
    "* simply load the RandomForestClassifier model\n",
    "* fit model using X & y\n",
    "* predict y_pred, by passing in X_test to the model\n",
    "\n",
    "\n",
    "### Random Forests\n",
    "\n",
    "Random Forests (or Random Decision Forests) are an ensemble ML technique (comprised of multiple algorithms to increase performance) that is mainly used for classification or regression tasks - here we will use it for classification. Random Forests enjoys the simplicity that comes with decision trees, but are far more accurate and account for the overfitting that occurs from decision trees.\n",
    "\n",
    "The algorithm constructs a multitude of decision trees and outputs the most popular classification for all the trees (classification) or the mean prediction (regression). In our Titanic example, each tree spits out a class prediction (Survived, 1,0) and the most popular prediction across the trees becomes the model's prediction.\n",
    "\n",
    "For Random Forests to work well we need to use features of the data which will have good predictive power (not just random garbage!) and we need predictions made by individual trees to have a low correlation with each other. The wisdom of the crowd gives Random Trees its effectiveness over the single use of a decision tree.\n",
    "\n",
    "Bagging and Feature Randomness are used by the Random Forests Algorithm to build individual trees that have low correlation.\n",
    "\n",
    "#### Decision Trees\n",
    "\n",
    "The logic behind a decision tree is to look for a feature in the dataset that we can use to split the dataset into sub branches. We perform the split again, until a decision is made about the class of the data. At each split in the tree (Node), we aim to optimise the greatest possible difference between data in each branch and maximise the similarity between data in the same branch.\n",
    "\n",
    "For our observation (test data input), we query the observation at each node (which is a question that splits the data the \"best\"). We do this until we have reached a conclusion about the class of the input data. Individually, Decision Trees are simple, but not very accurate at classifying new data.\n",
    "\n",
    "#### Random Forests Algorithm\n",
    "\n",
    "* We create a decision tree by using a bootstrapped version of our dataset and selecting a random subset of the features at each node (from this random subset we choose the \"best\" [see above] feature to split on). \n",
    "* From the root node, we continue this random selection process until it is possible to classify an input using this tree.\n",
    "* We continue to create many more trees uing this method of bootstrapping the data and choosing nodes from a randomn subset of the columns/features.\n",
    "* Predictions are made by passing the input through all of the trees and taking the most popular class across all of the trees as the model's prediction.\n",
    "\n",
    "\n",
    "\n",
    "### pandas.get_dummies\n",
    "\n",
    "Dummy Variables are a way to turn categorical data into numerical data using One-Hot Encoding; therefore allowing you to compare numerical data with previously uncomparable categorical fields.\n",
    "\n",
    "#### Ex.\n",
    "\n",
    "Color:\\\n",
    "Red\\\n",
    "Green\\\n",
    "Blue\n",
    "\n",
    "Becomes \n",
    "\n",
    "R: G: B:\\\n",
    "0 1 0\\\n",
    "1 0 0\\\n",
    "0 0 1\n",
    "\n",
    "Where R, G, B become the 'dummy' variables\n",
    "\n",
    "get_dummies(df['']) is used to create a new dataframe\\\n",
    "get_dummies(df, columns='') is used to merge the dummies automatically with the original df\n",
    "\n",
    "\n",
    "Be aware of Dummy Variable Trap:\n",
    "Drop one dummy var from the df, as it can be derived from the remaining (n-1) - this avoids errors.\\\n",
    "Why? What is DVT?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train Set\n",
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
      "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Test Set\n",
      "   PassengerId  Pclass                                          Name     Sex  \\\n",
      "0          892       3                              Kelly, Mr. James    male   \n",
      "1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n",
      "2          894       2                     Myles, Mr. Thomas Francis    male   \n",
      "3          895       3                              Wirz, Mr. Albert    male   \n",
      "4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n",
      "\n",
      "    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n",
      "0  34.5      0      0   330911   7.8292   NaN        Q  \n",
      "1  47.0      1      0   363272   7.0000   NaN        S  \n",
      "2  62.0      0      0   240276   9.6875   NaN        Q  \n",
      "3  27.0      0      0   315154   8.6625   NaN        S  \n",
      "4  22.0      1      1  3101298  12.2875   NaN        S  \n",
      "Index(['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch',\n",
      "       'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_set = pd.read_csv(\"~/Documents/Kaggle/titanic/train.csv\")\n",
    "test_set = pd.read_csv(\"~/Documents/Kaggle/titanic/test.csv\")\n",
    "\n",
    "print(\"\\n\\nTrain Set\")\n",
    "print(train_set.head())\n",
    "print(train_set.columns)\n",
    "print(\"\\n\\n\\n\\nTest Set\")\n",
    "print(test_set.head())\n",
    "print(test_set.columns)\n",
    "\n",
    "\n",
    "# Do we need to normalize the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of women who survived: 0.7420382165605095\n",
      "% of men who survived: 0.18890814558058924\n"
     ]
    }
   ],
   "source": [
    "# What is the survival rate for men/women?\n",
    "\n",
    "women = train_set.loc[train_set.Sex == 'female'][\"Survived\"]\n",
    "rate_women = sum(women)/len(women)\n",
    "\n",
    "print(\"% of women who survived:\", rate_women)\n",
    "\n",
    "men = train_set.loc[train_set.Sex == 'male'][\"Survived\"]\n",
    "rate_men = sum(men)/len(men)\n",
    "\n",
    "print(\"% of men who survived:\", rate_men)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model using RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass  SibSp  Parch  Sex_female  Sex_male\n",
      "0         3      1      0           0         1\n",
      "1         1      1      0           1         0\n",
      "2         3      0      0           1         0\n",
      "3         1      1      0           1         0\n",
      "4         3      0      0           0         1\n",
      "..      ...    ...    ...         ...       ...\n",
      "886       2      0      0           0         1\n",
      "887       1      0      0           1         0\n",
      "888       3      1      2           1         0\n",
      "889       1      0      0           0         1\n",
      "890       3      0      0           0         1\n",
      "\n",
      "[891 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "y = train_set[\"Survived\"]\n",
    "\n",
    "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
    "\n",
    "X = pd.get_dummies(train_set[features])\n",
    "X_test = pd.get_dummies(test_set[features])\n",
    "\n",
    "print(X) # Categorical data has been One-Hot Encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\n",
    "model.fit(X, y)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "0            892         0\n",
      "1            893         1\n",
      "2            894         0\n",
      "3            895         0\n",
      "4            896         1\n",
      "..           ...       ...\n",
      "413         1305         0\n",
      "414         1306         1\n",
      "415         1307         0\n",
      "416         1308         0\n",
      "417         1309         0\n",
      "\n",
      "[418 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "output = pd.DataFrame({'PassengerId': test_set.PassengerId, 'Survived': y_pred})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Done"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
